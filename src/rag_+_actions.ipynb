{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "pC-PQoCxcKl6",
        "outputId": "b68e3913-f5e1-4e2f-c60d-27b08acf31bb"
      },
      "outputs": [],
      "source": [
        "!pip install -qU \\\n",
        "    nemoguardrails==0.4.0 \\\n",
        "    pinecone-client==2.2.2 \\\n",
        "    datasets==2.14.3 \\\n",
        "    openai==0.27.8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0709sN9cKl8"
      },
      "source": [
        "To begin, we need to setup our data and retrieval components for RAG. We'll start with a dataset that contains info on the recent Llama 2 models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "50e9287f47a44aa49e45a05870958848"
          ]
        },
        "id": "rDo76dejcKl8",
        "outputId": "9ba61244-709a-4c5c-e657-83e4d6c2437e"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "data = load_dataset(\n",
        "    \"jamescalam/llama-2-arxiv-papers-chunked\",\n",
        "    split=\"train\"\n",
        ")\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LuLjiD5ycKl8"
      },
      "outputs": [],
      "source": [
        "data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wzF9e3SVcKl8"
      },
      "outputs": [],
      "source": [
        "data = data.map(lambda x: {\n",
        "    'uid': f\"{x['doi']}-{x['chunk-id']}\"\n",
        "})\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5Zun9xprcKl8"
      },
      "outputs": [],
      "source": [
        "data = data.to_pandas()\n",
        "# drop irrelevant fields\n",
        "data = data[['uid', 'chunk', 'title', 'source']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "AuUszBFzcKl9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.environ.get(\"OPENAI_API_KEY\") or \"key\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mt0oF0_ScKl9"
      },
      "source": [
        "Now we can create embeddings like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4fLOSa59cKl9"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "\n",
        "embed_model_id = \"text-embedding-small\"\n",
        "\n",
        "res = openai.Embedding.create(\n",
        "    input=[\n",
        "        \"We would have some text to embed here\",\n",
        "        \"And maybe another chunk here too\"\n",
        "    ], engine=embed_model_id\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgzqzHM3cKl9",
        "outputId": "4dd0786c-3810-4304-94da-694e3473f82f"
      },
      "outputs": [],
      "source": [
        "res.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UhfV2PWcKl9",
        "outputId": "ec81d686-3acf-49a1-ab22-9d6e8d3fd547"
      },
      "outputs": [],
      "source": [
        "len(res['data'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PRI7VdRcKl9",
        "outputId": "7f0a2e9a-d93d-44e4-b4e4-706d5dfeac91"
      },
      "outputs": [],
      "source": [
        "len(res['data'][0]['embedding']), len(res['data'][1]['embedding'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5n8R94bCcKl-"
      },
      "source": [
        "Now we need a place to store these embeddings and enable a efficient vector search through them all. To do that we use Pinecone, we can get a [free API key](https://app.pinecone.io/) and enter it below where we will initialize our connection to Pinecone and create a new index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kl7WXGFxcKl-",
        "outputId": "ff1c0e90-78bb-460d-8ff6-0e8b4d50cdd9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pinecone import Pinecone\n",
        "\n",
        "# initialize connection to pinecone (get API key at app.pinecone.io)\n",
        "api_key = os.environ.get('PINECONE_API_KEY') or 'PINECONE_API_KEY'\n",
        "\n",
        "# configure client\n",
        "pc = Pinecone(api_key=api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Ej5LsWaBiFx"
      },
      "outputs": [],
      "source": [
        "from pinecone import ServerlessSpec\n",
        "\n",
        "cloud = os.environ.get('PINECONE_CLOUD') or 'aws'\n",
        "region = os.environ.get('PINECONE_REGION') or 'us-east-1'\n",
        "\n",
        "spec = ServerlessSpec(cloud=cloud, region=region)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e35m6GFHcKl-"
      },
      "outputs": [],
      "source": [
        "index_name = \"linkedin-rag-with-actions\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVh3zecHcKl-",
        "outputId": "d1627bea-aeca-4926-9975-6f70bb0c5165"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "if index_name not in pc.list_indexes().names():\n",
        "    pc.create_index(\n",
        "        index_name,\n",
        "        dimension=len(res['data'][0]['embedding']),\n",
        "        metric='cosine',\n",
        "        spec=spec\n",
        "    )\n",
        "    while not pc.describe_index(index_name).status['ready']:\n",
        "        time.sleep(1)\n",
        "\n",
        "# connect to index\n",
        "index = pc.Index(index_name)\n",
        "# view index stats\n",
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "378b3bf651324e8a89cc35dbc5deb75e"
          ]
        },
        "id": "m68xo07QcKl-",
        "outputId": "ec1b29f5-757b-46fc-e752-18d2c59d7512"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "batch_size = 100  # how many embeddings we create and insert at once\n",
        "\n",
        "for i in tqdm(range(0, len(data), batch_size)):\n",
        "    # find end of batch\n",
        "    i_end = min(len(data), i+batch_size)\n",
        "    batch = data[i:i_end]\n",
        "    # get ids\n",
        "    ids_batch = batch['uid'].to_list()\n",
        "    # get texts to encode\n",
        "    texts = batch['chunk'].to_list()\n",
        "    # create embeddings\n",
        "    res = openai.Embedding.create(input=texts, engine=embed_model_id)\n",
        "    embeds = [record['embedding'] for record in res['data']]\n",
        "    # create metadata\n",
        "    metadata = [{\n",
        "        'chunk': x['chunk'],\n",
        "        'source': x['source']\n",
        "    } for _, x in batch.iterrows()]\n",
        "    to_upsert = list(zip(ids_batch, embeds, metadata))\n",
        "    # upsert to Pinecone\n",
        "    index.upsert(vectors=to_upsert)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ncVjWiqBiFy"
      },
      "source": [
        "## RAG Functions for Guardrails"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZbgLMxAcKl-"
      },
      "source": [
        "Now that we've added all of our text data to the index let's create a \"retrieve function\" that will allow us to take a user query, retrieve relevant records, and return them for use by our LLM.\n",
        "\n",
        "_Note: all functions defined and used with Guardrails `generate_async` must also be async functions._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulmrh-RucKl-"
      },
      "outputs": [],
      "source": [
        "async def retrieve(query: str) -> list:\n",
        "    # create query embedding\n",
        "    res = openai.Embedding.create(input=[query], engine=embed_model_id)\n",
        "    xq = res['data'][0]['embedding']\n",
        "    # get relevant contexts from pinecone\n",
        "    res = index.query(xq, top_k=5, include_metadata=True)\n",
        "    # get list of retrieved texts\n",
        "    contexts = [x['metadata']['chunk'] for x in res['matches']]\n",
        "    return contexts\n",
        "\n",
        "\n",
        "async def rag(query: str, contexts: list) -> str:\n",
        "    print(\"> RAG Called\")  # we'll add this so we can see when this is being used\n",
        "    context_str = \"\\n\".join(contexts)\n",
        "    # place query and contexts into RAG prompt\n",
        "    prompt = f\"\"\"You are a helpful assistant, below is a query from a user and\n",
        "    some relevant contexts. Answer the question given the information in those\n",
        "    contexts. If you cannot find the answer to the question, say \"I don't know\".\n",
        "\n",
        "    Contexts:\n",
        "    {context_str}\n",
        "\n",
        "    Query: {query}\n",
        "\n",
        "    Answer: \"\"\"\n",
        "    # generate answer\n",
        "    res = openai.Completion.create(\n",
        "        engine=\"text-davinci-003\",\n",
        "        prompt=prompt,\n",
        "        temperature=0.0,\n",
        "        max_tokens=100\n",
        "    )\n",
        "    return res['choices'][0]['text']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOdY3doZBiFz"
      },
      "source": [
        "## Guardrails"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUb3FAqaBiFz"
      },
      "source": [
        "We now need to initialize our configs for Rails:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKTzE-lRBiFz"
      },
      "outputs": [],
      "source": [
        "yaml_content = \"\"\"\n",
        "models:\n",
        "- type: main\n",
        "  engine: openai\n",
        "  model: gpt-4o-mini\n",
        "\"\"\"\n",
        "\n",
        "rag_colang_content = \"\"\"\n",
        "# define limits\n",
        "define user ask politics\n",
        "    \"what are your political beliefs?\"\n",
        "    \"thoughts on the president?\"\n",
        "    \"left wing\"\n",
        "    \"right wing\"\n",
        "\n",
        "define bot answer politics\n",
        "    \"I'm a shopping assistant, I don't like to talk of politics.\"\n",
        "    \"Sorry I can't talk about politics!\"\n",
        "\n",
        "define flow politics\n",
        "    user ask politics\n",
        "    bot answer politics\n",
        "    bot offer help\n",
        "\n",
        "# define RAG intents and flow\n",
        "define user ask llama\n",
        "    \"tell me about llama 2?\"\n",
        "    \"what is large language model\"\n",
        "    \"where did meta's new model come from?\"\n",
        "    \"how to llama?\"\n",
        "    \"have you ever meta llama?\"\n",
        "\n",
        "define flow llama\n",
        "    user ask llama\n",
        "    $contexts = execute retrieve(query=$last_user_message)\n",
        "    $answer = execute rag(query=$last_user_message, contexts=$contexts)\n",
        "    bot $answer\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTMYxfWqcKl_"
      },
      "outputs": [],
      "source": [
        "from nemoguardrails import LLMRails, RailsConfig\n",
        "\n",
        "# initialize rails config\n",
        "config = RailsConfig.from_content(\n",
        "    colang_content=rag_colang_content,\n",
        "    yaml_content=yaml_content\n",
        ")\n",
        "# create rails\n",
        "rag_rails = LLMRails(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjhDEGxbcKl_"
      },
      "source": [
        "Remember! We need to register any actions that are used in the Colang config file, otherwise our rails have no idea how to `execute retrieve` or `execute rag`. We register both like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suCXG0JfcKl_"
      },
      "outputs": [],
      "source": [
        "rag_rails.register_action(action=retrieve, name=\"retrieve\")\n",
        "rag_rails.register_action(action=rag, name=\"rag\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIj2QY0vcKl_"
      },
      "source": [
        "Now let's try out our RAG agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "aNL1xLEQcKl_",
        "outputId": "e2fcdeee-f458-4c72-ad3f-47302ce64d95"
      },
      "outputs": [],
      "source": [
        "await rag_rails.generate_async(prompt=\"hello\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "CZ-fKq5eeUCh",
        "outputId": "96328b30-43d3-44f7-ff02-4cba3e214f5a"
      },
      "outputs": [],
      "source": [
        "await rag_rails.generate_async(prompt=\"tell me about llama 2\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "redacre",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
